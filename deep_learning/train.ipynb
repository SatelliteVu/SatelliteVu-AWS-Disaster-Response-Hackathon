{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12698c38-cf8b-4a16-9296-42459f0d13ff",
   "metadata": {},
   "source": [
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/SatelliteVu/SatelliteVu-AWS-Disaster-Response-Hackathon/blob/main/deep_learning/train.ipynb)\n",
    "\n",
    "In this notebook, we train a ResU-Net architecture with the fire spread data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47af74-b198-4bde-bf49-a86184220e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from datagen import get_dataset\n",
    "from config import common_config, dataset_config, training_config, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e725c845-c179-470b-aace-56d1e604e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "if not os.path.exists(common_config[\"INPUT_DIR\"]):\n",
    "    os.makedirs(common_config[\"INPUT_DIR\"])\n",
    "    \n",
    "if not os.path.exists(common_config[\"OUTPUT_DIR\"]):\n",
    "    os.makedirs(common_config[\"OUTPUT_DIR\"])\n",
    "\n",
    "input_data_dir = os.path.join(common_config[\"INPUT_DIR\"], \"data\")\n",
    "\n",
    "parent_model_dir = os.path.join(common_config[\"INPUT_DIR\"], \"parent_model\")\n",
    "model_pattern = parent_model_dir + \"/*.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53327c-d2ea-46d5-ad44-e570c697b5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the subsets for training and validaton.\n",
    "train_dataset = get_dataset(\n",
    "      input_data_dir + dataset_config[\"TRAIN_DATASET_PATTERN\"],\n",
    "      data_size=model_config[\"IMG_SIZE\"][0],\n",
    "      sample_size=model_config[\"IMG_SIZE\"][0],\n",
    "      batch_size=training_config[\"BATCH_SIZE\"],\n",
    "      num_in_channels=len(dataset_config[\"INPUT_FEATURES\"]),\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=True,\n",
    "      random_crop=False,\n",
    "      center_crop=False,\n",
    "      shuffle=True\n",
    "        )\n",
    "eval_dataset = get_dataset(\n",
    "      input_data_dir + dataset_config[\"EVAL_DATASET_PATTERN\"],\n",
    "      data_size=model_config[\"IMG_SIZE\"][0],\n",
    "      sample_size=model_config[\"IMG_SIZE\"][0],\n",
    "      batch_size=training_config[\"BATCH_SIZE\"],\n",
    "      num_in_channels=len(dataset_config[\"INPUT_FEATURES\"]),\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=True,\n",
    "      random_crop=False,\n",
    "      center_crop=False,\n",
    "      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ddaef6-c713-490d-bb6b-254dc9eefb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting for verification of input data\n",
    "\n",
    "train_inputs, train_labels = next(iter(train_dataset))\n",
    "\n",
    "TITLES = dataset_config[\"INPUT_FEATURES\"] + dataset_config[\"OUTPUT_FEATURES\"]\n",
    "\n",
    "n_rows=5\n",
    "n_features= train_inputs.shape[3]\n",
    "\n",
    "CMAP = colors.ListedColormap(['silver', 'orangered'])\n",
    "BOUNDS = [0., 1.]\n",
    "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "keys = dataset_config[\"INPUT_FEATURES\"]\n",
    "\n",
    "fig = plt.figure(figsize=(15,6.5))\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_features + 1):\n",
    "        plt.subplot(n_rows, n_features + 1, i * (n_features + 1) + j + 1)\n",
    "        if i == 0:\n",
    "            plt.title(TITLES[j], fontsize=13)\n",
    "        if j < n_features - 1:\n",
    "            plt.imshow(train_inputs[i, :, :, j], cmap='viridis')\n",
    "        if j == n_features - 1:\n",
    "            plt.imshow(train_inputs[i, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "        if j == n_features:\n",
    "            plt.imshow(train_labels[i, :, :, 0], cmap=CMAP, norm=NORM) \n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82942a34-1fb9-4093-9781-1e55aeb624f4",
   "metadata": {},
   "source": [
    "Create custom_objects and get model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e30c2d-d1b9-475a-b5f7-171ec54ca002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_resunet\n",
    "import model_satunet\n",
    "import glob\n",
    "import sys\n",
    "import keras\n",
    "from metrics import dice_coef, get_loss_function\n",
    "\n",
    "# Get loss function\n",
    "loss_function = get_loss_function(training_config[\"LOSS_FUNCTION_NAME\"])\n",
    "\n",
    "# Define model architecture\n",
    "if model_config[\"MODEL_NAME\"] == \"resunet\":\n",
    "    model = model_resunet.get_model([model_config[\"IMG_SIZE\"][0],model_config[\"IMG_SIZE\"][1],len(dataset_config[\"INPUT_FEATURES\"])])\n",
    "elif model_config[\"MODEL_NAME\"] == \"satunet\":\n",
    "    model = model_satunet.get_model([model_config[\"IMG_SIZE\"][0],model_config[\"IMG_SIZE\"][1],len(dataset_config[\"INPUT_FEATURES\"])], num_layers=model_config[\"NB_LAYERS\"])\n",
    "else:\n",
    "    sys.exit(\"Provided wrong model name\")\n",
    "\n",
    "# Check if an input parent model was added to parent model directory\n",
    "parent_model_paths = glob.glob(model_pattern)\n",
    "\n",
    "if not len(parent_model_paths) <= 1:\n",
    "    print(\"Only one parent model can be added to the parent model directory\")\n",
    "    sys.exit()\n",
    "\n",
    "# If a parent model was given, load weights into the model\n",
    "if model_config[\"TRAIN_FROM_PARENT_MODEL\"] == True:\n",
    "    print(\"Loading parent model weights\")\n",
    "    # Load the parent model's weights\n",
    "    model.load_weights(parent_model_paths[0])\n",
    "    parent_model_name = parent_model_paths[0]\n",
    "else:    \n",
    "    parent_model_name = None\n",
    "\n",
    "# Unfreeze encoder layers if necessary\n",
    "if model_config[\"UNFREEZE_ALL_LAYERS\"]:\n",
    "    for i in range(len(model.layers)): \n",
    "        keras.layers.trainable = True\n",
    "        \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e4708-ca51-4069-b2d0-f9dc79a0ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "callbacks = list()\n",
    "\n",
    "# Optional: WandB callback config and init\n",
    "config = {\n",
    "    \"dataset_id\": dataset_config[\"DATASET_ID\"],\n",
    "    \"img_size\": model_config[\"IMG_SIZE\"],\n",
    "    \"model_architecture\": model_config[\"MODEL_NAME\"],\n",
    "    \"num_layers_satunet\": model_config[\"NB_LAYERS\"],\n",
    "    \"unfreeze_all_layers\": model_config[\"UNFREEZE_ALL_LAYERS\"],\n",
    "    \"parent_model_name\": parent_model_name,\n",
    "    \"optimizer\": training_config[\"OPTIMIZER_NAME\"],\n",
    "    \"learning_rate\": training_config[\"INITIAL_LEARNING_RATE\"],\n",
    "    \"loss_function\": training_config[\"LOSS_FUNCTION_NAME\"],\n",
    "    \"epochs\": training_config[\"NB_EPOCHS\"],\n",
    "    \"batch_size\": training_config[\"BATCH_SIZE\"],\n",
    "    \"custom_objects\": [\n",
    "        \"dice_coef\",\n",
    "        \"focal_tversky_loss\"\n",
    "        ],\n",
    "    \"input_features\": dataset_config[\"INPUT_FEATURES\"]\n",
    "    }\n",
    "wandb.init(project=\"fire-model\", config=config)\n",
    "run_name = wandb.run.name\n",
    "callbacks.append(WandbCallback())\n",
    "\n",
    "# Define learning rate schedule callback\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    training_config[\"INITIAL_LEARNING_RATE\"], decay_steps=15, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "\n",
    "# Define checkpoints callback\n",
    "checkpoint_path = os.path.join(common_config[\"OUTPUT_DIR\"], \"model\", \"fire_model_{}.h5\".format(run_name))\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, save_weights_only=True, save_best_only=True\n",
    "    )\n",
    "callbacks.append(checkpoint_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9726d-cd72-49d2-886f-f83b62068718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "if training_config[\"OPTIMIZER_NAME\"] == \"adam\":\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "elif training_config[\"OPTIMIZER_NAME\"] == \"nadam\":\n",
    "    optimizer = tf.keras.optimizers.Nadam()\n",
    "elif training_config[\"OPTIMIZER_NAME\"] == \"rmsprop\":\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "elif training_config[\"OPTIMIZER_NAME\"] == \"sgd\":\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "else:\n",
    "    sys.exit(\"Wrong optimizer name provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28705d9b-e224-4752-9dc1-14ab9f5c2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_function, metrics=[dice_coef,\n",
    "                                 tf.keras.metrics.AUC(curve=\"PR\"),\n",
    "                                 tf.keras.metrics.Precision(),\n",
    "                                 tf.keras.metrics.Recall()\n",
    "                                ]\n",
    "    )\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=eval_dataset,\n",
    "    epochs=training_config[\"NB_EPOCHS\"],\n",
    "    callbacks=callbacks\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow:Python",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
